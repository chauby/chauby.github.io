---
layout: post
title: Daily Paper (2023.1.23 - 2023.?.?)
categories: [Updating]
description: Some interesting papers
keywords: Computer Vision, Spatial Transformer, DAT, MOSE, Switch-NeRF, MRM, ToMe, HexPlane, Simple Multi-Scale Attention, Size-Adaptive Local Attention, PaLI, PDB-ConvLSTM
---

### [123_Attend Refine Repeat_Active Box Proposal Generation via In-Out Localization](http://arxiv.org/abs/1606.04446)

### [123_Cascade R-CNN_Delving into High Quality Object Detection](http://arxiv.org/abs/1712.00726)

### [123_FaceNet_A Unified Embedding for Face Recognition and Clustering](http://arxiv.org/abs/1503.03832)

### [123_Focal Loss for Dense Object Detection](http://arxiv.org/abs/1708.02002)

### [124_DeepLung_Deep 3D Dual Path Nets for Automated Pulmonary Nodule Detection and Classification](http://arxiv.org/abs/1801.09555)

### [124_Evaluate the Malignancy of Pulmonary Nodules Using the 3D Deep Leaky Noisy or Network](https://ieeexplore.ieee.org/document/8642524/)

### [124_Lung Nodule Classification using Deep Local-Global Networks](http://link.springer.com/10.1007/s11548-019-01981-7)

### [125_Co-Learning Feature Fusion Maps from PET-CT Images of Lung Cancer](https://ieeexplore.ieee.org/document/8737963/)

### [125_DIAGNOSTIC CLASSIFICATION OF LUNG NODULES USING 3D NEURAL NETWORKS](https://ieeexplore.ieee.org/document/8363687/)

### [125_Gated-Dilated Networks for Lung Nodule Classification in CT scans](https://ieeexplore.ieee.org/document/8930524/)

### [126_Identity Mappings in Deep Residual Networks](http://arxiv.org/abs/1603.05027)

### [126_SCA-CNN Spatial and Channel-Wise Attention in Convolutional Networks for Image Captioning](http://ieeexplore.ieee.org/document/8100150/)

### [127_Zero-Shot Visual Recognition Using Semantics-Preserving Adversarial Embedding Networks](https://ieeexplore.ieee.org/document/8578213/)

### [128_CROSSFORMER_A VERSATILE VISION TRANSFORMER HINGING ON CROSS_SCALE ATTENTION](https://arxiv.org/abs/2108.00154)

### [129_Classification-Then-Grounding_rEFORMULATING vIDEO sCENE gRAPHS AS tEMPORAL bIPARTITE gRAPHS](https://ieeexplore.ieee.org/document/9879749/)

### [129_Rethinking the Two-Stage Framework for Grounded Situation Recognition](https://ojs.aaai.org/index.php/AAAI/article/view/20167)

### [130_On Pursuit of Designing Multi-modal Transformer for Video Grounding](https://aclanthology.org/2021.emnlp-main.773)

### [131_AttentionGAN_ Unpaired Image-to-Image Translation using Attention-Guided Generatice Adversarial Networks](http://arxiv.org/abs/1911.11897)

### [131_PAD-Net_Multi-Tasks Guided Prediction-and-Distillation Network for Simultaneous Depth Estimation and Scene Parsing_paper](https://ieeexplore.ieee.org/document/8578175/)

### [21_Depth-Aware Generative Adversarial Network for Talking Head Video Generation](https://ieeexplore.ieee.org/document/9879781/)

### [21_DetCLIP_ Dictionary-Enriched Visual-Concept Paralleled Pre-Traing for Open-World Detection](http://arxiv.org/abs/2209.09407)

### [21_InvPT_Inverted Pyramid Multi-task Transformer for Dense Scene Understanding](http://arxiv.org/abs/2203.07997)

### [21_Multi-class Token Transformer for Weakly Supervised Semantic Segmentation](https://ieeexplore.ieee.org/document/9879800/))

### [22_Generalized Binary Search Network for Highly-Efficient Multi-View Stereo](https://ieeexplore.ieee.org/document/9880099/)

### [23_Efficient_Fused-Attention_Model_for_Steel_Surface_Defect_Detection](https://ieeexplore.ieee.org/document/9777969/)

### [23_Geometry-Aware_Facial_Expression_Recognition_via_Attentive_Graph_Convolutional_Networks](https://ieeexplore.ieee.org/document/9454388/)

### [23_IEEE_EMBS_Webinar_Series](https://sites.google.com/view/ieee-biip-webinars/)

### [23_Joint_Spine_Segmentation_and_Noise_Removal_From_Ultrasound_Volume_Projection_Images_With_Selective_Feature_Sharing](https://ieeexplore.ieee.org/document/9684363/)

### [24_Automatic Gesture Recognition in Robot-assisted Surgery with Reinforcement Learning and Tree Search](http://arxiv.org/abs/2002.08718)

### [25_Fully Convolutional Networks for Panoptic Segmentation](http://arxiv.org/abs/2012.00720)

### [26_ConDinst_Conditional Convolutions for Instance Segmentation](http://arxiv.org/abs/2003.05664)

### [26_Rethink before Releasing your Model_ML Model Extraction Attack in EDA](https://dl.acm.org/doi/10.1145/3566097.3567896)

### [27_3D-Pruning_A_Model_Compression_Framework_for_Efficient_ 3D_Action_Recognition](https://ieeexplore.ieee.org/document/9852466/)

### [27_APSNet_Toward_Adaptive_Point_Sampling_for_Efficient_3D _Action_Recognition](https://ieeexplore.ieee.org/document/9844448/)

### [28_Extracting Training Data from Large Language Models](http://arxiv.org/abs/2012.07805)

### [28_Optical Convolutional Neural Networks – Combining Silicon Photonics and Fourier Optics for Computer Vision](http://arxiv.org/abs/2108.05607)

### [29_Deformable ConvNets v2_More Deformable, Better Results](http://arxiv.org/abs/1811.11168)

- This paper was introduced in the previous blog.

### [29_LeNet_Gradient-based_learning_applied_to_document_recognition](http://ieeexplore.ieee.org/document/726791/)

- This paper was introduced in the previous blog.

### [210_Spatial Transformer Networks](https://arxiv.org/abs/1506.02025)

- This paper proposes a plug-and-play module, **Spatial transformer**, to handle the spatial manipulation of data within the network. As mentioned in the previous blog, CNN cannot be spatially invariant to the input data. This module is proposed to cope with this problem. This module has the invariance of translation, scaling, and rotation. CNNs with spatial transformers can perform better in multiple tasks, like image classification, co-localization, and spatial attention.

- The spatial transformer comprises three parts, **Localisation Network, Parameterised Sampling Grid, and Differentiable Image Sampling**.  
Localisation network takes the input feature map U $\in R^{H\times W \times C}$ and outputs $\theta=f_{loc}(U)$. Generally speaking, $f_{loc}()$ can be any form that should include a final regression layer to produce the transformation parameters $\theta$.  
Parameterized Sampling Grid aims to obtain the corresponding output coordinations to input coordinations. ($x_i^s$; $y_i^s$)'(Source coordinates in the input feature map) = $T_\theta(G_i)$ = $A_\theta$(Affine transformation matrix) $(x_i^t; y_i^t; 1)'$(Target coordinates of the regular grid in the output feature map) = $[\theta_{11} \theta_{12} \theta_{13}; \theta_{21} \theta_{22} \theta_{23}]$($x_i^t; y_i^t; 1$). This part provides the corresponding location of output for the next part. The examples of this part are shown below.

![Examples of Parameterised Sampling Grid](/images/DailyPaper/03/1.jpg "Examples of Parameterised Sampling Grid")  

Differentiable Image Sampling calculated the gray value of corresponding points in the expected sampling kernel. For example, the bilinear sampling kernel is: $V_i^c=\sum_n^H \sum_m^W U_{nm}^c max(0, 1-|x_i^s - m|) max(0, 1-|y_i^s - n|)$  
The combination of these three parts forms the complete spatial transformer, and the overall architecture is shown below. The following experiment is performed on the MNIST dataset and is tested based on FCN and CNN. Finally, the ST-CNN received SOTA results.

![Architecture of Spatial Transformer](/images/DailyPaper/03/2.jpg "Architecture of Spatial Transformer")

### [211_Vision Transformer with Deformable Attention](https://arxiv.org/pdf/2201.00520.pdf)

- This paper proposes **Deformable Attention Transformer(DAT)** which combines transformer with deformable module. DAT is the deformable self-attention backbone for visual recognition. I chose this paper because this idea is similar to the thinking when I read the deformable convolution paper. However, it is a pity that some people have produced a successful model. The receptive fields and queries of ViT, Swin Transformer, DCN, and DAT are below. It can be seen that DAT merges the characteristics of ViT and DCN.

![ViT, Swin Transformer, DCN and DAT](/images/DailyPaper/03/3.png "ViT, Swin Transformer, DCN and DAT")

- The Deformable Attention Mechanism is shown below. In part (a), a set of reference points uniformly distributed on the feature map is used. Then the offset values are learned by an offset network in part(b), and the offset is applied to the reference points. After that, bilinear interpolation is applied to find the offset coordinates on the input feature map. A small relevant part of the feature map is extracted as the source of k and v using a bilinear interpolation from the nearest four points around the weighted average. The detailed equations could be learned from the paper.

![Deformable Attention Mechanism](/images/DailyPaper/03/4.png "Deformable Attention Mechanism")

- The architecture of DAT is shown below. DAT achieves better performance than the Swin transformer in the following experiment. In the ablation part, the deformable attention only in the third and fourth stages will perform better than Swin Transformer.
  
![Architecture of DAT](/images/DailyPaper/03/5.png "Architecture of DAT")

### [212_MOSE: A New Dataset for Video Object Segmentation in Complex Scenes](https://arxiv.org/abs/2302.01872)

- Dataset: **comMplex video Object SEgmentation (MOSE)**.  
This dataset contains complex classification and recognition tasks, such as the disapperance-reapperance of objects, small/inconspicuous objects, heavy occlusions, and crowded environments. This paper creates the MOSE dataset and benchmarks 18 existing Video Object Segmentation(VOS) methods under four settings on the MOSE dataset to perform the comparison experiments. The details will not be discussed here, but there is one thing for sure: the MOSE dataset is a new challenge for the current video object segmentation methods because of the poor performance of the existing methods. As mentioned in the paper, *'We find that we are still at a nascent stage of segmenting and tracking objects in complex scenes where crowds, disappearing, occlusions, and inconspicuous/small objects occur frequently'*.

- Moreover, this paper proposes five potential future directions: **Stronger Association to Track Reappearing Objects, Video Object Segmentation of Occluded Objects, Attention on Small & Inconspicuous Objects, Tracking Objects in Crowd, and Long-Term Video Segmentation**. I want to solve some of them.

### [213_SWITCH-NERF_LEARNING SCENE DECOMPOSITION WITH MIXTURE OF EXPERTS FOR LARGE-SCALE NEURAL RADIANCE FIELDS ](https://openreview.net/forum?id=PQ2zoIZqvm)

- This paper proposes **Switch-NeRF** to handle the generalization, poor-learnable, and inconsistency problems. NeRF is short for Neural Radiance Fields, which is applied to reconstruct building-scale and even city-scale scenes. A sparsely Gated Mixture of Experts(MoE) is applied to optimize the NeRF sub-networks. The overall structure of Switch-NeRF is shown below. In this architecture, the Top-1 function on gate values G(x) normalized by Softmax is applied to sparsely select only one expert $E_s$ from a set of NeRF experts. Then the final output E(x) will be $G(x)_s E_s(x)$. The Gating network architecture is also shown below. In this architecture, G(x) equals Softmax(Linear(S(x))), and S(x) is an internal feature of a sub-network.

![Structure of Switch-NeRF](/images/DailyPaper/03/6.jpeg "Structure of Switch-NeRF")

![Structure of Gating Network](/images/DailyPaper/03/7.jpeg "Structure of Gating Network")

### [214_Advance Radiograph Representation Learning with Masked Record Modeling](https://arxiv.org/abs/2302.01872)

- This paper proposes **a unified framework based on Masked Record Modeling(MRM)** for radiograph representation learning to explore the direction of merging self-supervision and associated reports. MRM learns and repairs the masks in reports and radiographs to improve performance. With MRM pre-training, the radiography models could be trained more straightforwardly, faster, and better. The overview of the training strategy is shown below. Firstly, mask some input radiographs and report texts. Second, use the features reconstructed by addition merging to reconstruct the report texts. Thirdly, use the features of images to reconstruct images.

![MRM](/images/DailyPaper/03/8.jpeg "MRM's Training Stradegy")

- In the following experiment, this method outperforms other methods in RSNA Pneumonia and SIIM. Moreover, this method uses 1% labeled data to achieve the performance of other methods with 100% labeled data. In the medical graph image analysis, this is entirely meaningful.

### [215_Token Merging: Your ViT But Faster](https://arxiv.org/pdf/2210.09461)

- This paper proposes **Token Merging(ToMe)** to increase the throughput of the existing ViT model without needing to train. Since pruning token could lose information and needs re-training, ToMe combines tokens and increases the speed. The basic idea of using ToMe is to insert this module between the attention module and MLP in the ViT, as shown below (b). If there are L blocks and each block merges r tokens, it will reduce rL tokens. To find the similar tokens as shown below (a), the average of **keys(K)** features in different heads and **cosine** function are used to measure the similarity when performing the ablation experiment. Besides, $A = softmax(\frac{QK^T}{\sqrt d} + log s)$ is used to adjust the attention weight, and s is a row vector containing the size of each token. Moreover, this paper proposes **Bipartite Soft Matching** to merge tokens, as shown below (c). Firstly, partition the tokens into two sets, A and B, of roughly equal size. Secondly, draw one edge from each token in A to its most similar token in B. Thirdly, keep the r most similar edges. Fourthly, merge tokens that are still connected (e.g., by averaging their features). Finally, concatenate the two sets back together. The matching algorithm here is used to merge the tokens accurately rather than the clustering, which can not accurately decrease the tokens. In the following experiment part, the participation of ToMe improves the model's performance.

![ToMe](/images/DailyPaper/03/9.jpeg "ToMe")


### [216_HexPlane: A Fast Representation for Dynamic Scenes](https://arxiv.org/pdf/2301.09632)

- This paper raises **HexPlane**, which represents the dynamic 3D scenes by six planes of the learned feature. This method is fast than the NeRF method. Pairing with a tiny MLP, the workflow of HexPlane is shown below. The idea of HexPlane is simple, and it decomposes a 4D spacetime grid into six feature planes spanning each pair of coordinate axes, computes a feature vector for 4D points in spacetime by projecting the point onto each feature plane, then aggregating the six resulting feature vectors. Then a tiny MLP is used to predict.

![Work Flow of HexPlane](/images/DailyPaper/03/10.jpeg "Work Flow of HexPlane")

- The method overview of HexPlane is shown below. The details of the formulations will not be shown here. Since HexPlane divides the observation plane into six planes, the memory consumption can be adjusted in each plane. Therefore, HexPlane can save memory consumption. In conclusion, HexPlane alleviates the large memory consumption and sparse detection problem in dynamic scene reconstruction.

![HexPlane](/images/DailyPaper/03/11.jpeg "HexPlane")


### [217_Model-Agnostic Hierarchical Attention for 3D Object Detection](https://arxiv.org/abs/2301.02650)

- This paper proposes **two novel attention mechanisms** as modulized hierarchical designs for transformer-based 3D detectors. One is called **Simple Multi-Scale Attention**, which builds multi-scale tokens from a single-scale input feature to enable feature learning at different scales. The other is called **Size-Adaptive Local Attention**, with adaptive attention ranges for every bounding box proposal to localize feature aggregation. These two modules are plug-and-play. The illustration of Simple Multi-Scale Attention and Size-Adaptive Local Attention is shown below. With these two modules, the transformer could improve the ability to detect small objects.

![Illustration of Simple Multi-Scale Attention and Size-Adaptive Local Attention](/images/DailyPaper/03/12.png "Illustration of Simple Multi-Scale Attention and Size-Adaptive Local Attention")

- The Simple Multi-Scale Attention part uses two scales of features as key and value in the cross-attention between the target candidate and other points. Furthermore, multi-scale feature aggregation is implemented by multi-scale token aggregation, where different scales of key and value are used in different subsets of attention heads. Therefore, a higher-resolution feature map could be built with richer geometric detail for the point cloud. Size-Adaptive Local attention is used to build the hierarchical architecture by defining local areas. First, the middle-bounding box suggestion is generated with the features of the target candidate. Then a cross-attention function is performed between each candidate and the points sampled from within its corresponding box suggestion. Thus, the size-adaptive local region for each query point could be customized. Also, a max N local is set. If the bounding box is over N local, it will be randomly cropped into N local. If the bounding box is less than N local, it will be a padded token to achieve N local. This strategy is helpful for generalization.

### [218_PaLI: A Jointly-Scaled Multilingual Language-Image Model](https://arxiv.org/abs/2209.06794)

- This paper proposes **Pathways Language and Image Model(PaLI)**, which generates text based on visual and textual inputs, and with this interface, performs many vision, language, and multi-modal tasks in many languages. Moreover, joint scaling of the vision and language components is essential. This paper converts multiple image and language tasks into generalized VQA-like tasks to share knowledge among them. All tasks are constructed using "image + query to answer," where both retrieval and answer are represented as text tokens. This model allows PaLI to use cross-task transfer learning and enhance language-and-image comprehension in various visual and linguistic problems: image description, visual question, and answer, scene text understanding, etc. An example of a PaLI task is shown below.

![Example of PaLI Task](/images/DailyPaper/03/13.png "Example of PaLI Task")

- The architecture of PaLI is simple, which is shown below. The architecture uses an encoder-decoder Transformer model with a large-capacity ViT component for image processing. To provide visual tokens to the text encoder as input: the visual transformer takes the image and outputs the relevant features. Pooling is not applied to the output of the visual transformer until the visual token is passed to the encoder-decoder model via cross-attention. Besides, the training dataset uses WebLI. From my perspective, the most accurate model in the future will be enormous, which is hard to develop by a poor team. The wealthy company or something else will still handle the most accurate model.

![Architecture of PaLI](/images/DailyPaper/03/14.png "Architecture of PaLI")

### [219_Pseudo Pyramid Deeper Bidirectional ConvLSTM for Video Saliency Detection](https://openaccess.thecvf.com/content_ECCV_2018/papers/Hongmei_Song_Pseudo_Pyramid_Deeper_ECCV_2018_paper.pdf)

- This paper proposes a fast video salient object detection model based on a novel recurrent network architecture named **Pyramid Dilated Bidirectional ConvLSTM (PDB-ConvLSTM)**. This network is designed for video saliency object detection. A **Pyramid Dilated Convolution (PDC) module** is first designed for simultaneously extracting spatial features at multiple scales. PDC can further improve the spatial learning ability of DB-ConvLSTM. These spatial features are then concatenated and fed into an extended **Deeper Bidirectional ConvLSTM (DB-ConvLSTM)** to learn spatiotemporal information. This LSTM architecture can capture video sequences' long and short-term memory, contains both temporal and spatial information, and enhances the information exchange between LSTM units in bi-directions. The overall architecture of PDB-ConvLSTM is shown below.

![Architecture of PDB-ConvLSTM](/images/DailyPaper/03/15.png "Architecture of PDB-ConvLSTM")

- The PDC module is shown below. It consists of convolution layers with different dilation ratios to emphasize multi-scale spatial saliency representation learning. PDC lets the network automatically learn the weights of different features, which is more intuitive and effective than ASPP.

![PDC Module](/images/DailyPaper/03/16.png "PDC Module")

- The ConvLSTM and DB-ConvLSTM modules are shown below. ConvLSTM introduces convolution operation into input-to-state and state-to-state transitions. ConvLSTM preserves spatial information as well as models temporal dependency. The Bidirectional ConvLSTM (B-ConvLSTM) should capture temporal characteristics in bi-directions since ConvLSTM only remembers past sequences. However, in B-ConvLSTM, there is no information exchange between the forward and backward directional LSTM units. Therefore, Deeper Bidirectional ConvLSTM (DB-ConvLSTM) improves B-ConvLSTM by organizing the forward and backward ConvLSTM units in a cascaded and tighter way. Moreover, to extract more powerful spatiotemporal information and let the network adapt to salient targets at different scales, DB-ConvLSTM with a PDC-like structure(PDB-ConvLSTM) is presented. It can utilize different receptive fields' features to capture more complementary spatiotemporal features. So this is the evolution process of PDB-ConvLSTM.

![ConvLSTM and DB-ConvLSTM](/images/DailyPaper/03/17.png "ConvLSTM and DB-ConvLSTM")

### [220_SEARCHING FOR ACTIVATION FUNCTIONS](https://arxiv.org/pdf/1710.05941.pdf)

- This paper proposes **Swish(f(x)=x$\times$sigmoid($\beta x$))** which may has the better performance than ReLU. The activation function plays a major role in the success of training deep neural networks. The better activation function will contribute to better neural networks. An automatic searching technique is used in the discovery process.

- Swish = x$\sigma(\beta x)$, $\sigma(z)=(1+exp(-z)^{-1})$, $\beta$ is either a constant or a trainable parameter. The first derivative of Swish is f'(x) = $\beta f(x)$ + $\sigma(\beta x)$(1 - $\beta$f(x)). The figures of Swish and the first derivatives of Swish are shown below. Swish has no upper bound, lower bound, smooth, or non-monotonic properties. In the following experiment, the model with Swish rather than ReLU performs better in some datasets.

![Swish](/images/DailyPaper/03/18.png "Swish")

### [221_GestureGAN for Hand Gesture-to-Gesture Translation in the Wild](https://arxiv.org/abs/1808.04859)

- This paper proposes **hand Gesture Generative Adversarial Network(GestureGAN)** to solve the problem of translate the gesture in the wild. The model adopts the color loss and consistency loss to help evaluate the gesture. Moreover, Frechet ResNet Distance(FRD) is utilized to evaluate the quality of generated images. The piprline of GestureGAN is shown below.

![Pipeline of GestureGAN](/images/DailyPaper/03/19.jpeg "Pipeline of GestureGAN")

- The overall loss function of GestureGAN is L=$L_S(G,D,S_x,S_y)$ + $\lambda_1 L_{color{1,2}}(G,S_x,S_y)$ + $\lambda_2 L_{cyc}(G,S_x,S_y)$ + $\lambda_3 L_{identity}(G,S_x,S_y)$. The cycle loss is introduced as the consistency between source images and reconstructed image. The color loss is used to evaluate the possibility of artifacts. FRD's equation is FRD(x, y) = $\frac{1}{N}$ $\sum_1^N inf_{\alpha, \beta}$ max{d(f($x_i$)($\alpha(t)$)), f($y_i$)($\beta(t)$)}. FRD can evaluate the distance from the semantical level which is more accurate than FID. The proposed FRD is a measure of similarity between the feature vector of the real image f ($y_i$) and the feature vector of the generated image f ($x_i$) by calculating the Fréchet distance between them. The Fréchet distance is defined as the minimum cord-length sufficient to join a point traveling forward along f($y_i$) and one traveling forward along f ($x_i$), although the rate of travel for each point may not necessarily be uniform.

### [222_From Face to Natural Image_Learning Real Degradation for Blind Image Super-Resolution](https://web.comp.polyu.edu.hk/cslzhang/paper/ECCV22-ReDegNet.pdf)

- This paper propose **ReDegNet** which uses the real-world low-quality(LQ) face images and their restored high-quality(HQ) counterparts to model the complex real-world degradation. If ReDegNet can learn the real degradation process from face images then the knowledge can transfer these degradation representations from face to natural images to synthesize the degraded LQ natural images. Actually, this paper achieves this proposed result. Since the face and non-face (natural) regions in an image share the same degradation, if the model can learn the degradation process from the face part of the image, then this knowledge can reflect on other parts in the image. The overview of ReDegNet is shown below. From the overview, it can learn that the ReDegNet contains two sub-network: *$F_{deg},F_{syn}$*. Firstly, learn real degradation from face image. Secondly, synthesize the LQ images. Thirdly, transfer degradation to natural image. Fourthly, learn objective by disentanglement loss, and reconstruction loss.

![Overview of ReDegNet](/images/DailyPaper/03/20.png "Overview of ReDegNet")


### [222_Efficient and Degradation-Adaptive Network for Real-World Image Super-Resolution](https://web.comp.polyu.edu.hk/cslzhang/paper/ECCV2022_DASR.pdf)

- This paper proposes **Degradation-Adaptive Super-Resolution(DASR)** whose parameters are adaptively specified by estimating the degradation of each input image. DASR consists of a tiny regression network to estimate the degradation parameters of the input image and multiple light-weight super-resolution experts, which are jointly optimized on a balanced degradation space. DASR can help get rid of the enormous backbone. The overview of DASR is shown below. It can see that DASR consists of a degradation prediction network and a CNN-based SR network with multiple experts. The SR network is degradation-adaptive, this network boosts the model capacity via non-linear mixture of experts(MoE). This can effectively decrease the computation cost. Moreover, DASR only needs one adaptive network $E_A$ in the inference stage which can largely decrease the computation cost rather than the traditional MoE method. Besides, another advantage of DASR over other Real-ISR methods is that it supports easy user-interactive super-resolution during inference, owing to its interpretable and compact degradation representation. The super-resolution effects, such as blur-related parameters and the level of noise ,can be flexible adjusted, this is very helpful for the adjustment in practical Real-ISR tasks. The example of user-interactive super-resolution is shown after the overview.

![Overview of DASR](/images/DailyPaper/03/21.png "Overview of DASR")

![Example of User-interactive Super-resolution](/images/DailyPaper/03/22.png "Example of User-interactive Super-resolution")

### [223_You Only Train Once: Learning General and Distinctive 3D Local Descriptors](https://ieeexplore-ieee-org.lib.ezproxy.hkust.edu.hk/stamp/stamp.jsp?tp=&arnumber=9792207)

- I love this title which reminds me of the YOLO.

- This paper proposes **SpinNet**. This network is used to extract local surface descriptors which are rotation-invariant whilst sufficiently distinctive and general. This network can be decompose into three parts: **A Spatial Point Transformer** which is introduced to embed the input local surface into an elaborate cylindrical representation and further enabling end-to-end optimization of the entire framework,  **A Neural Feature Extractor** which composed of point-based and 3D cylindrical convolutional layers and presented to learn representative and general geometric patterns. and **An invariant layer** which is used to generate rotation-invariant feature descriptors. SpinNet has three properties:  
(1) It is rotation-invariant. Particularly, it learns consistent local features from 3D scans with different rotational angles.  
(2) It is descriptive. In essence, it preserves the prominent local patterns despite the noise, possible surface incompleteness, or different point densities.  
(3) It does not include any handcrafted features. Instead, it only consists of multiple point transformations and simple neural layers coupled with true end-to-end optimization.  

- Spatial Point Transformation is shown below. And the Neural Feature Extractor is shown in the following.

![Spatial Point Transformer](/images/DailyPaper/03/23.png "Spatial Point Transformer")

![Neural Feature Extractor](/images/DailyPaper/03/24.png "Neural Feature Extractor")

- The visualization of rotation equivariance and invariance is shown below. As mentioned in the experiment part, the dataset with rotation-based data augmentation will have the poor effect. If I have few data and want to train this neural network, how can I perform data augmentation to train the network to achieve the good performance?(Flip, Scale, Crop, Translation, Gaussian Noise, GAN, Interpolation in 2D)

![Rotation Equivariance and Invariance](/images/DailyPaper/03/25.png "Rotation Equivariance and Invariance")

### [223_Promising or Elusive? Unsupervised Object Segmentation from Real-world Single Images](https://arxiv.org/abs/2210.02324)

- This paper studies the problem of unsupervised object segmentation from single images by introducing four complexity factors to quantitatively measure the distributions of object-level and scene-level biases in appearance and geometry for datasets with human annotations. And this paper suggests that future work should exploit more explicit objectness biases in the network design. The four questions are (1) All training images do not have any human annotations.  
(2) Every single image has multiple objects.  
(3) Each image is treated as a static data point without any dynamic or temporal information.  
(4) All models are trained from scratch without requiring any pretrained networks on additional datasets.

- In object-level, this paper defines **Object Color Gradient and Gradient Shape Concavity** to evaluate the complexity of appearance and geometry respectively. In scene-level complexity, this paper introduces **Inter-object Color Similarity and Inter-object Shape Variation** to quantify the complexity of relative appearance and geometry between objects in an image. These four are complexity factors used to evaluate. The ablation experiment part is very worth to learn.

- The conclusions of this paper are:  
(1) Existing unsupervised object segmentation models cannot discover generic objects from single real-world images, although they can achieve outstanding performance on synthetic datasets.  
(2) The challenging distributions of both object- and scene-level biases in appearance and geometry from real-world images are the key factors incurring the failure of existing models.  
(3) the inductive biases introduced in existing unsupervised models are fundamentally not matched with the objectness biases exhibited in real-world images, and therefore fail to discover the real objectness.  
Or as imposed in the conclusion part, the distributions of both object- and scene-level biases in appearance and geometry of real-world datasets are particularly diverse and indiscriminative, such that current unsupervised models cannot segment real objects.

### [224_Measurement of full-field displacement time history of a vibrating continuous edge from video](https://www.sciencedirect.com/science/article/abs/pii/S0888327020302338?via%3Dihub)

- This paper proposes a method to **measure full-field displacement response of a vibrating continuous edge of a structural component**. The proposed method reveals the absolute as well as relative displacement response of the cable along with the dominating frequencies of the cable’s vibration. This is to develop the method of non-contact based vibration measuring sensors to alleviate the problems of cost and maintenance. The measurement of the full-field displacement time history of an edge from the video of a vibrating structure refers to the acquisition of displacement time history of each pixel of the edge with sub-pixel accuracy by tracking its motion in subsequent frames both in the horizontal and vertical direction in the plane of the camera. This is very meaning for avoiding tragedies like bridge cable breakage.

### [224_Full-field structural monitoring using event cameras and physics-informed sparse identification](https://www.sciencedirect.com/science/article/abs/pii/S0888327020302910?via%3Dihub)

- This paper proposes a framework called **physics-informed sparse identification for full-field structural vibration tracking and analysis**. This framework is to fit the event cameras whose output is event. The event camera is introduced for perceiving structural motion, with high spatial and temporal resolution. Since the output is different from the traditional camera, the algorithm for applying is also different. The better application on event camera will bring many advantage to the structure detection.

### [225_All One Needs to Know about Priors for Deep Image Restoration and Enhancement_A Survey](https://arxiv.org/abs/2206.02070)

- This paper introduce **recent advancements of priors for deep image restoration and enhancement**. Image restoration and enhancement can be divided into six types: (1) Environmental influence removal, which aims to remove the artifacts from images caused by the environmental factors; (2) Camera processing pipeline, or image signal processor (ISP), which converts the photoelectric signals into digital signals; (3) Deblurring, which aims to restore sharp images by removing blur artifacts mainly caused by object or camera motion; (4) Compression artifacts removal, which aims to reduce the low-quality problems caused by lossy compression in the image storage and transmission process; (5) Super-resolution (SR), which aims to enhance the resolution of an image or video; (6) Video frame interpolation, or super slow motion, which aims to convert a low-frame-rate video into a high-frame-rate video. This paper and the follow paper are good literature review about the related fields.

### [225_Deep Learning for HDR Imaging_SOTA and Future Trends](https://ieeexplore.ieee.org/document/9594668/)

- This paper discusses **High Dynamic Range (HDR) imaging** on five categories: (1) Number/Domain of input exposures; (2) Number of learning tasks; (3) Novel sensor data; (4) Novel learning strategies; and (5) Applications. The methods of deep multi-exposure HDR imaging are shown below. Moreover, there still exists some problems need to be solved in HDR imaging, such as, the need for large-scale dataset, model size, requirements for the equipment, the loss of generalization, and the environment issue when introduced into the actual world.

![Deep Multi-exposure HDR Imaging Methods](/images/DailyPaper/03/28.png "Deep Multi-exposure HDR Imaging Methods")

### [225_Efficient Video Deblurring Guided by Motion Magnitude](https://link.springer.com/10.1007/978-3-031-19800-7_24)

- This paper **uses motion magnitude prior(MMP), which consists of both spatial and temporal blur level information and can be integrated into an efficient RNN, as guidance** for to perform video deblurring. Firstly, use the average magnitude of optical flow from the high-frequency sharp frames to generate the synthetic blurry frames and their corresponding pixel-wise motion magnitude maps. THen build a dataset including the blurry frame and MMP pairs. A compact CNN learned the MMP by regression. The value of MMP positively correlated to the level of motion blur. An efficient RNN with residual dense blocks (RDBs) as backbone video deblurring network. Three components are introduced to utilize MMP better: (a) A motion magnitude attentive module (MMAM) is proposed to inject the MMP into the network  for the intra-frame utilization. (b) Different from other RNNbased methods which only pass the deblurred features to the next frame, the features before deblurring are passed to the next frame for the inter-frame utilization. The blurry frame with pixels of different blur level is weighted by the MMAM, as such, pixels under low blur level can be directly utilized by the next frame. (c) The motion magnitude of network output can reflect the deblur performance for loss-level utilization. It means that the sharper the image is, the lower the average score is. The structure of MMP is shown below. And the structure of MMP-RNN is shown in the following.

![Motion Magnitude Prior](/images/DailyPaper/03/26.png "Motion Magnitude Prior")

![MMP-RNN](/images/DailyPaper/03/27.png "MMP-RNN")

### [226_Align before Fuse_Vision and Language Representation Learning with Momentum Distillation](https://arxiv.org/pdf/2107.07651.pdf)

### [226_BLIP_Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](https://arxiv.org/pdf/2201.12086.pdf)

### [226_BLIP-2_Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/pdf/2301.12597.pdf)

### [226_NeX_Real-time View Synthesis with Neural Basis Expansion_CVPR2021_Oral](https://nex-mpi.github.io/)

### [227_NDF_Neural Deformable Fields for Dynamic Human Modelling](https://arxiv.org/pdf/2207.09193v1.pdf)

### [227_A Dual-Masked Auto-Encoder for Robust Motion Capture with Spatial-Temporal Skeletal Token Completion](https://arxiv.org/pdf/2207.07381.pdf)

### [227_ChoreoGraph_Music-conditioned Automatic Dance Choreography over a Style and Tempo Consistent Dynamic Graph](https://arxiv.org/pdf/2207.07386.pdf)