---
layout: post
title: Daily Paper (2023.1.23 - 2023.?.?)
categories: [Updating]
description: Some interesting papers
keywords: Computer Vision
---

### [123_Attend Refine Repeat_Active Box Proposal Generation via In-Out Localization](http://arxiv.org/abs/1606.04446)

### [123_Cascade R-CNN_Delving into High Quality Object Detection](http://arxiv.org/abs/1712.00726)

### [123_FaceNet_A Unified Embedding for Face Recognition and Clustering](http://arxiv.org/abs/1503.03832)

### [123_Focal Loss for Dense Object Detection](http://arxiv.org/abs/1708.02002)

### [124_DeepLung_Deep 3D Dual Path Nets for Antomated Pulmonary Nodule Detection and Classification](http://arxiv.org/abs/1801.09555)

### [124_Evaluate the Malignancy of Pulmonary Nodules Using the 3D Deep Leaky Noisy or Network](https://ieeexplore.ieee.org/document/8642524/)

### [124_Lung Nodule Classification using Deep Local-Global Networks](http://link.springer.com/10.1007/s11548-019-01981-7)

### [125_Co-Learning Feature Fusion Maps from PET-CT Images of Lung Cancer](https://ieeexplore.ieee.org/document/8737963/)

### [125_DIAGNOSTIC CLASSIFICATION OF LUNG NODULES USING 3D NEURAL NETWORKS](https://ieeexplore.ieee.org/document/8363687/)

### [125_Gated-Dilated Networks for Lung Nodule Classification in CT scans](https://ieeexplore.ieee.org/document/8930524/)

### [126_Identity Mappings in Deep Residual Networks](http://arxiv.org/abs/1603.05027)

### [126_SCA-CNN Spatial and Channel-Wise Attention in Convolutional Networks for Image Captioning](http://ieeexplore.ieee.org/document/8100150/)

### [127_Zero-Shot Visual Recognition Using Semantics-Preserving Adversarial Embedding Networks](https://ieeexplore.ieee.org/document/8578213/)

### [128_CROSSFORMER_A VERSATILE VISION TRANSFORMER HINGING ON CROSS_SCALE ATTENTION](https://arxiv.org/abs/2108.00154)

### [129_Classification-Then-Grounding_rEFORMULATING vIDEO sCENE gRAPHS AS tEMPORAL bIPARTITE gRAPHS](https://ieeexplore.ieee.org/document/9879749/)

### [129_Rethinking the Two-Stage Framework for Grounded Situation Recognition](https://ojs.aaai.org/index.php/AAAI/article/view/20167)

### [130_On Pursuit of Designing Multi-modal Transformer for Video Grounding](https://aclanthology.org/2021.emnlp-main.773)

### [131_AttentionGAN_ Unpaired Image-to-Image Translation using Attention-Guided Generatice Adversarial Networks](http://arxiv.org/abs/1911.11897)

### [131_PAD-Net_Multi-Tasks Guided Prediction-and-Distillation Network for Simultaneous Depth Estimation and Scene Parsing_paper](https://ieeexplore.ieee.org/document/8578175/)

### [21_Depth-Aware Generative Adversarial Network for Talking Head Video Generation](https://ieeexplore.ieee.org/document/9879781/)

### [21_DetCLIP_ Dictionary-Enriched Visual-Concept Paralleled Pre-Traing for Open-World Detection](http://arxiv.org/abs/2209.09407)

### [21_InvPT_Inverted Pyramid Multi-task Transformer for Dense Scene Understanding](http://arxiv.org/abs/2203.07997)

### [21_Multi-class Token Transformer for Weakly Supervised Semantic Segmentation](https://ieeexplore.ieee.org/document/9879800/))

### [22_Generalized Binary Search Network for Highly-Efficient Multi-View Stereo](https://ieeexplore.ieee.org/document/9880099/)

### [23_Efficient_Fused-Attention_Model_for_Steel_Surface_Defect_Detection](https://ieeexplore.ieee.org/document/9777969/)

### [23_Geometry-Aware_Facial_Expression_Recognition_via_Attentive_Graph_Convolutional_Networks](https://ieeexplore.ieee.org/document/9454388/)

### [23_IEEE_EMBS_Webinar_Series](https://sites.google.com/view/ieee-biip-webinars/)

### [23_Joint_Spine_Segmentation_and_Noise_Removal_From_Ultrasound_Volume_Projection_Images_With_Selective_Feature_Sharing](https://ieeexplore.ieee.org/document/9684363/)

### [24_Automatic Gesture Recognition in Robot-assisted Surgery with Reinforcement Learning and Tree Search](http://arxiv.org/abs/2002.08718)

### [25_Fully Convolutional Networks for Panoptic Segmentation](http://arxiv.org/abs/2012.00720)

### [26_ConDinst_Conditional Convolutions for Instance Segmentation](http://arxiv.org/abs/2003.05664)

### [26_Rethink before Releasing your Model_ML Model Extraction Attack in EDA](https://dl.acm.org/doi/10.1145/3566097.3567896)

### [27_3D-Pruning_A_Model_Compression_Framework_for_Efficient_ 3D_Action_Recognition](https://ieeexplore.ieee.org/document/9852466/)

### [27_APSNet_Toward_Adaptive_Point_Sampling_for_Efficient_3D _Action_Recognition](https://ieeexplore.ieee.org/document/9844448/)

### [28_Extracting Training Data from Large Language Models](http://arxiv.org/abs/2012.07805)

### [28_Optical Convolutional Neural Networks – Combining Silicon Photonics and Fourier Optics for Computer Vision](http://arxiv.org/abs/2108.05607)

### [29_Deformable ConvNets v2_More Deformable, Better Results](http://arxiv.org/abs/1811.11168)

- This paper had been introduced in the previous blog.

### [29_LeNet_Gradient-based_learning_applied_to_document_recognition](http://ieeexplore.ieee.org/document/726791/)

- This paper had been introduced in the previous blog.

### [210_Spatial Transformer Networks](https://arxiv.org/abs/1506.02025)

- This paper proposes a plug-and-play module **Spatial Transformer** to handle with the spatial manipulation of data within the network. As mentioned in the previous blog, the CNN lacks of ability to be spatially invariant to the input data. This module is proposed to copy with this problem. This module has the invariance of translation, scaling and rotation. CNNs with spatial transformers can perform better in some multiple tasks, like image classification, co-localisation, and spatial attention.

- The spatial transformer is made up of three parts, **Localisation Network, Parameterised Sampling Grid and Differentiable Image Sampling**.  
Localisation network takes the input feature map U $\in R^{H\times W \times C}$ and outputs $\theta=f_{loc}(U)$. Generally speaking, $f_{loc}()$ can be any form which should finally include a final regression layer to produce the transformation parameters $\theta$.  
Parameterised Sampling Grid aims to obtain the corresponding output coordinations to input coordinations. ($x_i^s$; $y_i^s$)'(Source coordinates in the input feature map) = $T_\theta(G_i)$ = $A_\theta$(Affine transformation matrix) $(x_i^t; y_i^t; 1)'$(Target coordinates of the regular grid in the output feature map) = $[\theta_{11} \theta_{12} \theta_{13}; \theta_{21} \theta_{22} \theta_{23}]$($x_i^t; y_i^t; 1$). This part provides the corresponding location of output for the next part. The examples of this part is shown below.

![Examples of Parameterised Sampling Grid](/images/DailyPaper/03/1.jpg "Examples of Parameterised Sampling Grid")  

Differentiable Image Sampling calculated the gray value of corresponding points in the expected sampling kernel. For example, the bilinear sampling kernel is: $V_i^c=\sum_n^H \sum_m^W U_{nm}^c max(0, 1-|x_i^s - m|) max(0, 1-|y_i^s - n|)$  
The combination of these three part forms the complete spatial transformer, the overall architecture is shown below. The following experiment performs on MNIST dataset and is tested based on FCN and CNN. Finally the ST-CNN receive SOTA results.

![Architecture of Spatial Transformer](/images/DailyPaper/03/2.jpg "Architecture of Spatial Transformer")

### [211_Vision Transformer with Deformable Attention](https://arxiv.org/pdf/2201.00520.pdf)

- This paper proposes **Deformable Attention Transformer(DAT)** which combines transformer with deformable module. DAT is the deformable self-attention backbone for visual recognition. The reason why I chose this paper is that this idea is similar to the thinking when I read the deformable convolution paper. But it is a pity that some people had produced the successful model. The receptive fields and queries of ViT, Swin Transformer, DCN and DAT are below. It can be seen that DAT merges the characteristics of ViT and DCN.

![ViT, Swin Transformer, DCN and DAT](/images/DailyPaper/03/3.png "ViT, Swin Transformer, DCN and DAT")

- The Deformable Attention Mechanism is shown below. In part (a), a set of reference points uniformly distributed on the feature map is used. Then the offset values are learned by an offset network in part(b) and the offset is applied to the reference points. After that, bilinear interpolation is applied to find the offset coordinates on the input feature map. A small relevant part of the feature map is extracted as the original source of k and v using a bilinear interpolation from the nearest four points around the weighted average. The detail equations could be learned from the paper.

![Deformable Attention Mechanism](/images/DailyPaper/03/4.png "Deformable Attention Mechanism")

- The architecture of DAT is shown below. DAT achieves the better performance than swin transformer in the following experiment. In the ablation part, the deformable attention only in third and fourth stage will result in the better performance than Swin Transformer.
  
![Architecture of DAT](/images/DailyPaper/03/5.png "Architecture of DAT")

### [212_MOSE: A New Dataset for Video Object Segmentation in Complex Scenes](https://arxiv.org/abs/2302.01872)

- Dataset: **comMplex video Object SEgmentation (MOSE)**.  
  This dataset contains the complex classification and recognition tasks. For example the disapperance-reapperance of objects, small/inconspicuous object, heavy occlusions, crowded environments. This paper creates MOSE dataset and benchmarks 18 existing Video Object Segmentation(VOS) methods under 4 different settings on MOSE dataset to perform the comparison experiments. The details will not be discussed here, but there is one thing for certain: the MOSE dataset is new challenge for the current video object segmentation methods because of the poor performance of the existing methods. As mentioned in the paper, *'We find that we are still at a nascent stage of segmenting and tracking objects in complex scenes where crowds, disappearing, occlusions, and inconspicuous/small objects occur frequently'*.

- Moreover, this paper proposes five potential future directions: **Stronger Association to Track Reappearing Objects, Video Object Segmentation of Occluded Objects, Attention on Small & Inconspicuous Objects, Tracking Objects in Crowd, and Long-Term Video Segmentation**. I want to solve some of them if I have the chance in the future.

### [213_SWITCH-NERF_LEARNING SCENE DECOMPOSITION WITH MIXTURE OF EXPERTS FOR LARGE-SCALE NEURAL RADIANCE FIELDS ](https://openreview.net/forum?id=PQ2zoIZqvm)

- NeRF is short for Neural Radiance Fields, which is applied to reconstruct building-scale and even city-scale scenes. This paper proposes **Switch-NeRF** to handle the generalization, poor-learnable and inconsistency problems. Sparsely Gated Mixture of Experts(MoE) is applied tooptimize the the NeRF sub-networks. The overall structure of Switch-NeRF is shown below. In this architecture, the Top-1 function on gate values G(x) normalized by Softmax is applied to sparsely select only 1 expert $E_s$ from a set of NeRF experts. Then the final output E(x) will be $G(x)_s E_s(x)$. The Gating network architecture is also shown below. In this architecture, G(x) equals to Softmax(Linear(S(x))), S(x) is an internal feature of a sub-network.

![Structure of Switch-NeRF](/images/DailyPaper/03/6.jpeg "Structure of Switch-NeRF")

![Structure of Gating Network](/images/DailyPaper/03/7.jpeg "Structure of Gating Network")

### [214_Advance Radiograph Represenation Learning with Masked Record Modeling](https://arxiv.org/abs/2302.01872)

- This paper proposes **a unified framework based on Masked Record Modeling(MRM)** for radiograph representation learning to explore the direction of merging self-supervision and associated reports. MRM learns and repairs the masks in both reports and radiograph to improve the performance. With MRM pre-training, the radiography models could be trained simpler, faster, and better. The overview of the training stradegy is shown below. Firstly, mask some input radiographs and report texts. Seconding, use the features reconstructed by addtion merging to reconstruct the report texts. Thirdly, use the features of images to reconstruct images.

![MRM](/images/DailyPaper/03/8.jpeg "MRM's Training Stradegy")

- In the following experiment, this method outperforms than other methods in RSNA Pneumonia and SIIM. Moreover, this method use 1% labeled data to achieve the performance of other methods with 100% labeled data. In the medical graph image analysis, this is quite meaningful.

### [215_Token Merging: Your ViT But Faster](https://arxiv.org/pdf/2210.09461)

- This paper proposes **Token Merging(ToMe)** to increase the throughput of existing ViT model without needing to train. Since pruning token could lose information and needs re-training, ToMe is used to combine tokens and increase the speed. The basic idea of using ToMe is to insert this module into between the attention module and MLP in the ViT as shown blow(b). If there are L blocks and each block merges r token, then it will reduce rL tokens. To find the similar tokens as shown blow(a), the average of **keys(K)** features in different head and **cosine** function are used to measure the similarity when performing the ablation experiment. Besides, $A = softmax(\frac{QK^T}{\sqrt d} + log s)$ is used to adjust the attention weight, s is a row vector containing the size of each token. Moreover, this paper proposes **Bipartite Soft Matching** to merge tokens as shown blow(c). Firstly, partition the tokens into two sets A and B of roughly equal size. Secondly, draw one edge from each token in A to its most similar token in B. Thirdly, keep the r most similar edges. Fourthly, merge tokens that are still connected (e.g., by averaging their features). Finally, concatenate the two sets back together. Matching algorithm here is used to merge the tokens accurately rather than the clustering which can not accurately decrease the tokens. In the following experiment part, the participation of ToMe improves the model's performance.

![ToMe](/images/DailyPaper/03/9.jpeg "ToMe")


### [216_HexPlane: A Fast Representation for Dynamic Scenes](https://arxiv.org/pdf/2301.09632)

- This paper raises **HexPlane** whihc represents the dynamic 3D scenes by six planes of learned feature. This method is fast than the NeRF method. Pairing with a tiny MLP, the work flow of HexPlane is shown below. The idea of HexPlane is simple, it decomposes a 4D spacetime grid into six feature planes spanning each pair of coordinate axes, computes a feature vector for a 4D point in spacetime by projecting the point onto each feature plane, then aggregating the six resulting feature vectors. Then a tiny MLP is used to predict.

![Work Flow of HexPlane](/images/DailyPaper/03/10.jpeg "Work Flow of HexPlane")

- The method overview of HexPlane is shown below. The details of fomulations will not be shown here. Since HexPlane divides the observation plane into 6 planes, the memory consume can be adjusted respectively in each planes. Therefore, HexPlane can save the memory consuming. In conclusion, HexPlane alleviates the huge consuming of memory and sparse detection problem in dynamic scene reconstruction.

![HexPlane](/images/DailyPaper/03/11.jpeg "HexPlane")


### [217_Model-Agnostic Hierarchical Attention for 3D Object Detection](https://arxiv.org/abs/2301.02650)

- This paper propose **two novel attention mechanisms** as modulized hierarchical designs for transformer-based 3D detectors. One is called **Simple Multi-Scale Attention** that builds multi-scale tokens from a single-scale input feature to enable feature learning at different scales. The other one is called **Size-Adaptive Local Attention** with adaptive attention ranges for every bounding box proposal to localize feature aggregation. These two modules are plug-and-play. The illustration of Simple Multi-Scale Attention and Size-Adaptive Local Attention are shown below. With these two module, the transformer could improve the ability of detecting small objects.

![Illustration of Simple Multi-Scale Attention and Size-Adaptive Local Attention](/images/DailyPaper/03/12.png "Illustration of Simple Multi-Scale Attention and Size-Adaptive Local Attention")

- In Simple Multi-Scale Attention part, it uses two scales of features as key and value in the cross-attention between the target candidate and other points. Furthermore, multi-scale feature aggregation is implemented by multi-scale token aggregation, where different scales of key and value are used in different subsets of attention heads. Therefore, a higher resolution feature map could be builded with richer geometric detail for the point cloud. Size-Adaptive Local Attention is used to build the hierarchar architecture by defining local areas. First the middle bounding box suggestion is generated with the features of the target candidate. Then a cross-attention function is performed between each candidate and the points sampled from within its corresponding box suggestion. Thus, the size-adaptive local region for each query point could be customized. Also, a max N local is set. If the bounding box is over N local, then it will be randomly cropped into N local. If the bounding box is less than N local, then it will be padded token to achieve N local. This is useful for the generalization.

### [218_Emotional AI and the future of wellbeing in the post‑pandemic](https://link.springer.com/article/10.1007/s00146-023-01639-8?utm_source=researcher_app&utm_medium=referral&utm_campaign=RESR_MRKT_Researcher_inbound)

